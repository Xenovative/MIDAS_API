# Memory Optimization for Large Documents

## Problem

Out of memory (OOM) errors when uploading large documents due to:
1. **Large embeddings in memory** - 1536 floats per chunk
2. **Multiple batches in memory** - Accumulating data
3. **Large document content** - Stored in database
4. **Python garbage collection** - Not freeing memory fast enough

## Solutions Implemented

### ‚úÖ 1. Reduced Batch Size

**Before:**
```python
batch_size = 50  # 50 chunks √ó 1536 dimensions = ~300KB per batch
```

**After:**
```python
batch_size = 20  # 20 chunks √ó 1536 dimensions = ~120KB per batch
```

**Impact:**
- 60% less memory per batch
- More batches but safer
- Prevents OOM on large documents

### ‚úÖ 2. Explicit Memory Cleanup

**Added after each batch:**
```python
# Free memory after each batch
del chunk_texts      # Delete chunk text list
del embeddings       # Delete embedding vectors
del batch_chunks     # Delete chunk objects
gc.collect()         # Force garbage collection
```

**Impact:**
- Immediate memory release
- Prevents memory accumulation
- Forces Python to clean up

### ‚úÖ 3. Deferred Content Loading

**Database model optimization:**
```python
content = Column(Text, nullable=False, deferred=True)
```

**Impact:**
- Content not loaded unless explicitly accessed
- Saves memory when listing documents
- Only loads when needed

### ‚úÖ 4. File Size Warnings

**Added size checks:**
```python
file_size_mb = len(content) / (1024 * 1024)
if file_size_mb > 5:
    print(f"‚ö†Ô∏è  Large file: {file_size_mb:.2f}MB - may take several minutes")

text_size_mb = len(text_content) / (1024 * 1024)
if text_size_mb > 2:
    print(f"‚ö†Ô∏è  Large text: {text_size_mb:.2f}MB - processing will be slow")
```

**Impact:**
- Early warning for large files
- Users know what to expect
- Can cancel if too large

## Memory Usage Breakdown

### Per Chunk
```
Text content: ~1KB (1000 chars)
Embedding: ~6KB (1536 floats √ó 4 bytes)
Metadata: ~0.5KB
Total: ~7.5KB per chunk
```

### Per Batch (20 chunks)
```
Chunk texts: 20KB
Embeddings: 120KB
Metadata: 10KB
Total: ~150KB per batch
```

### Full Document (500 chunks)
```
Original text: ~500KB
All embeddings: ~3MB
Database storage: ~3.5MB
Peak memory during processing: ~500KB (one batch at a time)
```

## Memory Limits

### Safe Limits
- **File size**: < 5MB
- **Text content**: < 2MB
- **Chunks**: < 2000
- **Processing time**: < 10 minutes

### Warning Zone
- **File size**: 5-10MB
- **Text content**: 2-5MB
- **Chunks**: 2000-5000
- **Processing time**: 10-30 minutes

### Danger Zone (May OOM)
- **File size**: > 10MB
- **Text content**: > 5MB
- **Chunks**: > 5000
- **Processing time**: > 30 minutes

## Your 317KB PDF

### Analysis
```
File size: 0.31MB ‚úÖ Safe
Text content: ~0.32MB ‚úÖ Safe
Estimated chunks: ~320
Estimated batches: 16 (320 √∑ 20)
Peak memory: ~150KB per batch ‚úÖ Safe
Total time: ~2-3 minutes ‚úÖ Acceptable
```

**Verdict: Should work fine with new optimizations!**

## If Still Getting OOM

### Option 1: Further Reduce Batch Size
```python
batch_size=10  # Even smaller batches
```

**Trade-off:**
- More API calls
- Slower processing
- But safer memory-wise

### Option 2: Increase Chunk Size
```python
chunk_size=2000  # Larger chunks
```

**Trade-off:**
- Fewer chunks = less memory
- But worse retrieval quality
- Not recommended

### Option 3: Split Document
```
# Instead of one 500-page PDF:
- Chapter 1: pages 1-100
- Chapter 2: pages 101-200
- Chapter 3: pages 201-300
- etc.
```

**Benefits:**
- Smaller files = safer
- Can upload in parallel
- Better organization

### Option 4: Increase System Memory
```
# If running locally:
- Close other applications
- Increase Python memory limit
- Use 64-bit Python
```

## Monitoring Memory

### Backend Logs
```
üìÑ Processing document.pdf: 320 chunks
‚ö†Ô∏è  Large file detected: 5.2MB - this may take several minutes
‚ö†Ô∏è  Large text content: 2.3MB - processing will be slow
  ‚è±Ô∏è  Embedding generation took 8.23s for 20 chunks
  ‚úì Batch 1/16: 20/320 chunks (took 8.45s)
  [Memory cleanup after batch]
  ‚è±Ô∏è  Embedding generation took 7.89s for 20 chunks
  ‚úì Batch 2/16: 40/320 chunks (took 8.12s) (ETA: ~120s)
  [Memory cleanup after batch]
  ...
‚úÖ Added document 'document.pdf' with 320 chunks
```

### System Monitoring
```bash
# Windows
tasklist /FI "IMAGENAME eq python.exe"

# Linux/Mac
ps aux | grep python
top -p $(pgrep python)
```

## Configuration

### Current Settings (Optimized for Memory)
```python
# Vector Store
chunk_size = 1000          # Balanced
chunk_overlap = 200        # Minimal
batch_size = 20            # Reduced for memory

# Database
content = deferred=True    # Lazy loading

# Cleanup
gc.collect()              # After each batch
del variables             # Explicit deletion
```

### Alternative Settings (If Still OOM)
```python
# More aggressive memory saving
chunk_size = 1500          # Fewer chunks
chunk_overlap = 100        # Less overlap
batch_size = 10            # Smaller batches

# Even more aggressive
chunk_size = 2000
chunk_overlap = 0
batch_size = 5
```

## Best Practices

### ‚úÖ Do
- Upload during off-peak hours
- Close other applications
- Monitor memory usage
- Split very large documents
- Use recommended settings

### ‚ùå Don't
- Upload multiple documents simultaneously
- Use very small chunk sizes
- Process 1000+ page documents
- Ignore memory warnings
- Increase batch size too much

## Troubleshooting

### Error: "Out of Memory"
```
1. Check file size (should be < 5MB)
2. Check text content size (should be < 2MB)
3. Reduce batch_size to 10
4. Close other applications
5. Restart Python process
6. Try splitting document
```

### Error: "Process killed"
```
1. System ran out of memory
2. Reduce batch_size to 5
3. Increase system RAM
4. Use smaller documents
5. Process in chunks
```

### Slow but Working
```
‚úÖ This is normal for large documents
‚úÖ Watch the progress bar
‚úÖ Check ETA estimates
‚úÖ Be patient
```

## Performance vs Memory Trade-off

### Fast but Memory-Hungry
```python
batch_size = 50
chunk_size = 1000
# Pros: Faster (fewer API calls)
# Cons: More memory, may OOM
```

### Slow but Memory-Safe
```python
batch_size = 10
chunk_size = 1500
# Pros: Won't OOM, very safe
# Cons: Slower (more API calls)
```

### Balanced (Recommended)
```python
batch_size = 20
chunk_size = 1000
# Pros: Good speed, safe memory
# Cons: None (optimal)
```

## Summary

### What We Changed
- ‚úÖ Reduced batch size: 50 ‚Üí 20
- ‚úÖ Added memory cleanup after each batch
- ‚úÖ Deferred content loading in database
- ‚úÖ Added file size warnings
- ‚úÖ Explicit garbage collection

### Expected Results
- ‚úÖ Can handle 5MB files safely
- ‚úÖ Can process 2000+ chunks
- ‚úÖ No memory accumulation
- ‚úÖ Stable long-running uploads
- ‚úÖ Clear warnings for large files

### If Still Having Issues
1. Reduce batch_size to 10
2. Split large documents
3. Increase system memory
4. Monitor memory usage
5. Contact support with logs

---

**Status**: ‚úÖ Optimized for Memory
**Batch Size**: 20 (reduced from 50)
**Memory Cleanup**: Explicit after each batch
**Safe File Size**: < 5MB
**Recommendation**: Restart server and try again
